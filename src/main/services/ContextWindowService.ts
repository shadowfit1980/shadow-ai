/**
 * ðŸ’¬ ContextWindowService
 * 
 * Context window management:
 * - Truncation, summarization, chunking
 */

import { EventEmitter } from 'events';

export class ContextWindowService extends EventEmitter {
    private static instance: ContextWindowService;
    private constructor() { super(); }
    static getInstance(): ContextWindowService {
        if (!ContextWindowService.instance) {
            ContextWindowService.instance = new ContextWindowService();
        }
        return ContextWindowService.instance;
    }

    generate(): string {
        return `// Context Window Service - Truncation, summarization
// Generated by Shadow AI

import { encode, decode } from 'gpt-tokenizer';

class ContextWindowManager {
    private maxTokens: number;
    private reservedTokens: number;
    
    constructor(maxTokens = 128000, reservedTokens = 4000) {
        this.maxTokens = maxTokens;
        this.reservedTokens = reservedTokens;
    }
    
    // Count tokens
    countTokens(text: string): number {
        return encode(text).length;
    }
    
    // Truncate to fit
    truncate(text: string, maxTokens: number): string {
        const tokens = encode(text);
        if (tokens.length <= maxTokens) return text;
        
        return decode(tokens.slice(0, maxTokens));
    }
    
    // Fit messages in context window
    fitMessages(messages: Message[], systemPrompt: string): Message[] {
        const systemTokens = this.countTokens(systemPrompt);
        let availableTokens = this.maxTokens - this.reservedTokens - systemTokens;
        
        const result: Message[] = [];
        
        // Keep most recent messages, drop from the middle
        for (let i = messages.length - 1; i >= 0; i--) {
            const msgTokens = this.countTokens(messages[i].content);
            
            if (msgTokens <= availableTokens) {
                result.unshift(messages[i]);
                availableTokens -= msgTokens;
            } else if (result.length === 0) {
                // Always keep at least the last message, truncated
                result.unshift({
                    ...messages[i],
                    content: this.truncate(messages[i].content, availableTokens)
                });
                break;
            }
        }
        
        return result;
    }
    
    // Summarize long content
    async summarize(content: string, maxLength = 500): Promise<string> {
        if (this.countTokens(content) <= maxLength) return content;
        
        const response = await llm.chat([{
            role: 'system',
            content: \`Summarize the following content in \${maxLength} tokens or less. Preserve key information.\`
        }, {
            role: 'user',
            content: content
        }]);
        
        return response.content;
    }
    
    // Chunk content for processing
    chunk(content: string, chunkSize = 1000, overlap = 100): string[] {
        const tokens = encode(content);
        const chunks: string[] = [];
        
        for (let i = 0; i < tokens.length; i += chunkSize - overlap) {
            const chunk = tokens.slice(i, i + chunkSize);
            chunks.push(decode(chunk));
        }
        
        return chunks;
    }
    
    // Map-reduce for long content
    async mapReduce(content: string, mapPrompt: string, reducePrompt: string): Promise<string> {
        const chunks = this.chunk(content);
        
        // Map phase
        const mappedResults = await Promise.all(chunks.map(async (chunk) => {
            const response = await llm.chat([{
                role: 'system',
                content: mapPrompt
            }, {
                role: 'user',
                content: chunk
            }]);
            return response.content;
        }));
        
        // Reduce phase
        const response = await llm.chat([{
            role: 'system',
            content: reducePrompt
        }, {
            role: 'user',
            content: mappedResults.join('\\n---\\n')
        }]);
        
        return response.content;
    }
    
    // Estimate tokens for model selection
    estimateTokens(messages: Message[]): { total: number; canFitGPT4: boolean; canFitGPT35: boolean } {
        const total = messages.reduce((sum, m) => sum + this.countTokens(m.content), 0);
        
        return {
            total,
            canFitGPT4: total <= 128000,
            canFitGPT35: total <= 16000
        };
    }
}

export { ContextWindowManager };
`;
    }
}

export const contextWindowService = ContextWindowService.getInstance();
