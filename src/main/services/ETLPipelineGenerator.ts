/**
 * ðŸ”„ ETL Pipeline Generator
 * 
 * Generate data pipelines:
 * - Extract, Transform, Load
 */

import { EventEmitter } from 'events';

export class ETLPipelineGenerator extends EventEmitter {
    private static instance: ETLPipelineGenerator;

    private constructor() { super(); }

    static getInstance(): ETLPipelineGenerator {
        if (!ETLPipelineGenerator.instance) {
            ETLPipelineGenerator.instance = new ETLPipelineGenerator();
        }
        return ETLPipelineGenerator.instance;
    }

    generate(): string {
        return `// ETL Data Pipelines
// Generated by Shadow AI

// === Node.js ETL Pipeline ===
import { Transform, Readable, Writable, pipeline } from 'stream';
import { promisify } from 'util';
import csv from 'csv-parse';
import { stringify } from 'csv-stringify';

const pipelineAsync = promisify(pipeline);

// Extract: Read from source
async function* extractFromAPI(url: string) {
    let page = 1;
    let hasMore = true;
    
    while (hasMore) {
        const response = await fetch(\`\${url}?page=\${page}&limit=100\`);
        const data = await response.json();
        
        for (const item of data.items) {
            yield item;
        }
        
        hasMore = data.hasNextPage;
        page++;
    }
}

function extractFromCSV(filePath: string): Readable {
    return fs.createReadStream(filePath).pipe(
        csv.parse({ columns: true, skip_empty_lines: true })
    );
}

// Transform: Process data
class TransformRecord extends Transform {
    constructor(private transformFn: (record: any) => any) {
        super({ objectMode: true });
    }

    _transform(chunk: any, encoding: string, callback: Function) {
        try {
            const transformed = this.transformFn(chunk);
            if (transformed) {
                this.push(transformed);
            }
            callback();
        } catch (error) {
            callback(error);
        }
    }
}

function createTransformer(fn: (record: any) => any): Transform {
    return new TransformRecord(fn);
}

// Load: Write to destination
async function loadToDatabase(records: AsyncIterable<any>, batchSize = 1000) {
    let batch = [];
    
    for await (const record of records) {
        batch.push(record);
        
        if (batch.length >= batchSize) {
            await db.insert(batch);
            console.log(\`Inserted \${batch.length} records\`);
            batch = [];
        }
    }
    
    if (batch.length > 0) {
        await db.insert(batch);
        console.log(\`Inserted \${batch.length} records\`);
    }
}

function loadToCSV(outputPath: string): Writable {
    return pipeline(
        stringify({ header: true }),
        fs.createWriteStream(outputPath)
    );
}

// === Complete ETL Pipeline ===
interface ETLConfig {
    source: { type: 'api' | 'csv' | 'database'; config: any };
    transforms: Array<{ name: string; fn: (record: any) => any }>;
    destination: { type: 'database' | 'csv' | 's3'; config: any };
}

async function runETL(config: ETLConfig) {
    console.log('Starting ETL pipeline...');
    const startTime = Date.now();
    let recordCount = 0;

    // Extract
    let source: AsyncIterable<any>;
    switch (config.source.type) {
        case 'api':
            source = extractFromAPI(config.source.config.url);
            break;
        case 'csv':
            source = extractFromCSV(config.source.config.path);
            break;
        default:
            throw new Error(\`Unknown source type: \${config.source.type}\`);
    }

    // Transform
    async function* transformStream(input: AsyncIterable<any>) {
        for await (const record of input) {
            let transformed = record;
            
            for (const { name, fn } of config.transforms) {
                try {
                    transformed = fn(transformed);
                    if (!transformed) break; // Filter out
                } catch (error) {
                    console.error(\`Transform \${name} failed:\`, error);
                    break;
                }
            }
            
            if (transformed) {
                recordCount++;
                yield transformed;
            }
        }
    }

    // Load
    const transformedData = transformStream(source);
    
    switch (config.destination.type) {
        case 'database':
            await loadToDatabase(transformedData);
            break;
        case 's3':
            await uploadToS3(transformedData, config.destination.config);
            break;
    }

    const duration = (Date.now() - startTime) / 1000;
    console.log(\`ETL complete: \${recordCount} records in \${duration}s\`);
    
    return { recordCount, duration };
}

// === Example Usage ===
const etlConfig: ETLConfig = {
    source: {
        type: 'api',
        config: { url: 'https://api.example.com/users' }
    },
    transforms: [
        { name: 'normalize', fn: (r) => ({ ...r, email: r.email.toLowerCase() }) },
        { name: 'validate', fn: (r) => r.email.includes('@') ? r : null },
        { name: 'enrich', fn: (r) => ({ ...r, processedAt: new Date().toISOString() }) }
    ],
    destination: {
        type: 'database',
        config: { table: 'users' }
    }
};

runETL(etlConfig);

// === Scheduled Pipeline ===
import cron from 'node-cron';

cron.schedule('0 * * * *', async () => {
    console.log('Running hourly ETL sync...');
    await runETL(etlConfig);
});

export { extractFromAPI, extractFromCSV, createTransformer, loadToDatabase, runETL };
`;
    }
}

export const etlPipelineGenerator = ETLPipelineGenerator.getInstance();
