/**
 * ðŸ¤– LLM Integrator
 * 
 * Integrate LLMs:
 * - OpenAI, Anthropic, local models
 */

import { EventEmitter } from 'events';

export type LLMProvider = 'openai' | 'anthropic' | 'ollama' | 'groq';

export class LLMIntegrator extends EventEmitter {
    private static instance: LLMIntegrator;

    private constructor() { super(); }

    static getInstance(): LLMIntegrator {
        if (!LLMIntegrator.instance) {
            LLMIntegrator.instance = new LLMIntegrator();
        }
        return LLMIntegrator.instance;
    }

    getProviders(): LLMProvider[] {
        return ['openai', 'anthropic', 'ollama', 'groq'];
    }

    generateIntegration(): string {
        return `// LLM Integration
// Generated by Shadow AI

import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';

// Universal LLM Interface
interface LLMConfig {
    provider: 'openai' | 'anthropic' | 'ollama';
    model: string;
    apiKey?: string;
    baseUrl?: string;
}

interface Message {
    role: 'system' | 'user' | 'assistant';
    content: string;
}

interface CompletionOptions {
    maxTokens?: number;
    temperature?: number;
    topP?: number;
    stream?: boolean;
}

class UniversalLLM {
    private config: LLMConfig;
    private openai?: OpenAI;
    private anthropic?: Anthropic;

    constructor(config: LLMConfig) {
        this.config = config;
        this.initClient();
    }

    private initClient() {
        switch (this.config.provider) {
            case 'openai':
                this.openai = new OpenAI({
                    apiKey: this.config.apiKey || process.env.OPENAI_API_KEY,
                    baseURL: this.config.baseUrl
                });
                break;
            case 'anthropic':
                this.anthropic = new Anthropic({
                    apiKey: this.config.apiKey || process.env.ANTHROPIC_API_KEY
                });
                break;
            case 'ollama':
                this.openai = new OpenAI({
                    apiKey: 'ollama',
                    baseURL: this.config.baseUrl || 'http://localhost:11434/v1'
                });
                break;
        }
    }

    async chat(messages: Message[], options: CompletionOptions = {}) {
        const { maxTokens = 1024, temperature = 0.7, stream = false } = options;

        if (this.config.provider === 'anthropic') {
            const systemMessage = messages.find(m => m.role === 'system');
            const chatMessages = messages.filter(m => m.role !== 'system');

            const response = await this.anthropic!.messages.create({
                model: this.config.model,
                max_tokens: maxTokens,
                temperature,
                system: systemMessage?.content,
                messages: chatMessages.map(m => ({
                    role: m.role as 'user' | 'assistant',
                    content: m.content
                }))
            });

            return {
                content: response.content[0].type === 'text' ? response.content[0].text : '',
                usage: { input: response.usage.input_tokens, output: response.usage.output_tokens }
            };
        }

        // OpenAI / Ollama
        const response = await this.openai!.chat.completions.create({
            model: this.config.model,
            messages,
            max_tokens: maxTokens,
            temperature,
            stream
        });

        if (stream) {
            return response; // Return stream
        }

        return {
            content: response.choices[0].message.content || '',
            usage: response.usage
        };
    }

    async *streamChat(messages: Message[], options: CompletionOptions = {}) {
        const stream = await this.chat(messages, { ...options, stream: true });
        
        for await (const chunk of stream as any) {
            const content = chunk.choices[0]?.delta?.content;
            if (content) yield content;
        }
    }

    async embed(text: string | string[]) {
        if (this.config.provider === 'anthropic') {
            throw new Error('Anthropic does not support embeddings directly');
        }

        const input = Array.isArray(text) ? text : [text];
        const response = await this.openai!.embeddings.create({
            model: 'text-embedding-3-small',
            input
        });

        return response.data.map(d => d.embedding);
    }
}

// Specialized AI Functions
class AIHelpers {
    private llm: UniversalLLM;

    constructor(config: LLMConfig) {
        this.llm = new UniversalLLM(config);
    }

    async summarize(text: string, maxWords = 100) {
        const { content } = await this.llm.chat([
            { role: 'system', content: \`You are a summarization assistant. Summarize in \${maxWords} words or less.\` },
            { role: 'user', content: text }
        ]);
        return content;
    }

    async translate(text: string, targetLanguage: string) {
        const { content } = await this.llm.chat([
            { role: 'system', content: \`Translate the following text to \${targetLanguage}. Only output the translation.\` },
            { role: 'user', content: text }
        ]);
        return content;
    }

    async generateCode(prompt: string, language: string) {
        const { content } = await this.llm.chat([
            { role: 'system', content: \`You are an expert \${language} programmer. Generate clean, well-commented code.\` },
            { role: 'user', content: prompt }
        ]);
        return content;
    }

    async analyzeDocument(document: string, question: string) {
        const { content } = await this.llm.chat([
            { role: 'system', content: 'Analyze the document and answer questions about it accurately.' },
            { role: 'user', content: \`Document:\\n\${document}\\n\\nQuestion: \${question}\` }
        ]);
        return content;
    }

    async extractStructuredData(text: string, schema: object) {
        const { content } = await this.llm.chat([
            { 
                role: 'system', 
                content: \`Extract data from text into JSON matching this schema: \${JSON.stringify(schema)}. Only output valid JSON.\`
            },
            { role: 'user', content: text }
        ], { temperature: 0 });
        
        return JSON.parse(content);
    }
}

// React Hook
function useAI(config: LLMConfig) {
    const [loading, setLoading] = useState(false);
    const [error, setError] = useState<Error | null>(null);
    const llm = useMemo(() => new UniversalLLM(config), [config]);

    const chat = useCallback(async (messages: Message[], options?: CompletionOptions) => {
        setLoading(true);
        setError(null);
        try {
            return await llm.chat(messages, options);
        } catch (err) {
            setError(err as Error);
            throw err;
        } finally {
            setLoading(false);
        }
    }, [llm]);

    return { chat, loading, error };
}

export { UniversalLLM, AIHelpers, useAI };
`;
    }
}

export const llmIntegrator = LLMIntegrator.getInstance();
