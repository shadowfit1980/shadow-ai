/**
 * üîç RAG Pipeline Generator
 * 
 * Retrieval-Augmented Generation:
 * - Vector databases
 * - Embeddings
 * - Semantic search
 */

import { EventEmitter } from 'events';

export type VectorDB = 'pinecone' | 'weaviate' | 'chroma' | 'pgvector';

export class RAGPipelineGenerator extends EventEmitter {
    private static instance: RAGPipelineGenerator;

    private constructor() { super(); }

    static getInstance(): RAGPipelineGenerator {
        if (!RAGPipelineGenerator.instance) {
            RAGPipelineGenerator.instance = new RAGPipelineGenerator();
        }
        return RAGPipelineGenerator.instance;
    }

    getVectorDatabases(): VectorDB[] {
        return ['pinecone', 'weaviate', 'chroma', 'pgvector'];
    }

    generatePipeline(): string {
        return `// RAG Pipeline
// Generated by Shadow AI

import { OpenAI } from 'openai';
import { Pinecone } from '@pinecone-database/pinecone';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';

const openai = new OpenAI();
const pinecone = new Pinecone();

interface Document {
    id: string;
    content: string;
    metadata: Record<string, any>;
}

interface SearchResult {
    id: string;
    score: number;
    content: string;
    metadata: Record<string, any>;
}

class RAGPipeline {
    private indexName: string;
    private namespace: string;
    private embeddingModel = 'text-embedding-3-small';
    private chatModel = 'gpt-4-turbo-preview';

    constructor(indexName: string, namespace = 'default') {
        this.indexName = indexName;
        this.namespace = namespace;
    }

    // Ingest documents
    async ingestDocuments(documents: Document[]) {
        const splitter = new RecursiveCharacterTextSplitter({
            chunkSize: 1000,
            chunkOverlap: 200,
        });

        const index = pinecone.Index(this.indexName);
        const chunks = [];

        for (const doc of documents) {
            const splits = await splitter.splitText(doc.content);
            
            for (let i = 0; i < splits.length; i++) {
                chunks.push({
                    id: \`\${doc.id}-chunk-\${i}\`,
                    content: splits[i],
                    metadata: {
                        ...doc.metadata,
                        documentId: doc.id,
                        chunkIndex: i
                    }
                });
            }
        }

        // Generate embeddings in batches
        const batchSize = 100;
        for (let i = 0; i < chunks.length; i += batchSize) {
            const batch = chunks.slice(i, i + batchSize);
            const embeddings = await this.embedTexts(batch.map(c => c.content));
            
            const vectors = batch.map((chunk, j) => ({
                id: chunk.id,
                values: embeddings[j],
                metadata: { content: chunk.content, ...chunk.metadata }
            }));

            await index.namespace(this.namespace).upsert(vectors);
        }

        return { chunksProcessed: chunks.length };
    }

    // Semantic search
    async search(query: string, topK = 5): Promise<SearchResult[]> {
        const queryEmbedding = await this.embedText(query);
        const index = pinecone.Index(this.indexName);
        
        const results = await index.namespace(this.namespace).query({
            vector: queryEmbedding,
            topK,
            includeMetadata: true
        });

        return results.matches?.map(match => ({
            id: match.id,
            score: match.score || 0,
            content: match.metadata?.content as string || '',
            metadata: match.metadata || {}
        })) || [];
    }

    // RAG query
    async query(question: string, contextLimit = 5): Promise<{
        answer: string;
        sources: SearchResult[];
    }> {
        const sources = await this.search(question, contextLimit);
        
        if (sources.length === 0) {
            return { answer: "I don't have enough context to answer this question.", sources: [] };
        }

        const context = sources.map((s, i) => \`[\${i + 1}] \${s.content}\`).join('\\n\\n');

        const response = await openai.chat.completions.create({
            model: this.chatModel,
            messages: [
                {
                    role: 'system',
                    content: \`You are a helpful assistant that answers questions based on the provided context. 
                              Always cite your sources using [1], [2], etc.
                              If the context doesn't contain relevant information, say so.\`
                },
                {
                    role: 'user',
                    content: \`Context:\\n\${context}\\n\\nQuestion: \${question}\`
                }
            ],
            temperature: 0.7
        });

        return {
            answer: response.choices[0].message.content || '',
            sources
        };
    }

    // Conversational RAG
    async chat(messages: Array<{ role: string; content: string }>, contextLimit = 5) {
        const lastUserMessage = messages.filter(m => m.role === 'user').pop()?.content || '';
        const sources = await this.search(lastUserMessage, contextLimit);
        
        const context = sources.length > 0
            ? sources.map((s, i) => \`[\${i + 1}] \${s.content}\`).join('\\n\\n')
            : 'No relevant context found.';

        const systemMessage = {
            role: 'system' as const,
            content: \`You are a helpful assistant. Use this context to answer questions:
                      \\n\${context}\\n
                      Cite sources as [1], [2], etc when using information from the context.\`
        };

        const response = await openai.chat.completions.create({
            model: this.chatModel,
            messages: [systemMessage, ...messages.map(m => ({
                role: m.role as 'user' | 'assistant',
                content: m.content
            }))]
        });

        return {
            message: response.choices[0].message.content || '',
            sources
        };
    }

    // Embedding helpers
    private async embedText(text: string): Promise<number[]> {
        const response = await openai.embeddings.create({
            model: this.embeddingModel,
            input: text
        });
        return response.data[0].embedding;
    }

    private async embedTexts(texts: string[]): Promise<number[][]> {
        const response = await openai.embeddings.create({
            model: this.embeddingModel,
            input: texts
        });
        return response.data.map(d => d.embedding);
    }

    // Delete documents
    async deleteDocuments(documentIds: string[]) {
        const index = pinecone.Index(this.indexName);
        
        // Delete all chunks for these documents
        // Note: Pinecone requires knowing chunk IDs
        for (const docId of documentIds) {
            // This requires listing and deleting by prefix or metadata filter
            await index.namespace(this.namespace).deleteMany({
                filter: { documentId: { '\$eq': docId } }
            });
        }
    }
}

// Hybrid Search (combining keyword and semantic)
class HybridRAG extends RAGPipeline {
    async hybridSearch(query: string, topK = 5) {
        // Semantic search
        const semanticResults = await this.search(query, topK * 2);
        
        // Keyword boost
        const keywords = query.toLowerCase().split(/\\s+/);
        
        const reranked = semanticResults.map(result => {
            const content = result.content.toLowerCase();
            const keywordMatches = keywords.filter(kw => content.includes(kw)).length;
            const keywordBoost = keywordMatches / keywords.length * 0.2;
            
            return {
                ...result,
                score: result.score + keywordBoost
            };
        });

        return reranked
            .sort((a, b) => b.score - a.score)
            .slice(0, topK);
    }
}

export { RAGPipeline, HybridRAG };
`;
    }
}

export const ragGenerator = RAGPipelineGenerator.getInstance();
