/**
 * üéôÔ∏è VoiceGenerator
 * 
 * Voice features:
 * - Speech-to-text, text-to-speech, commands
 */

import { EventEmitter } from 'events';

export class VoiceGenerator extends EventEmitter {
    private static instance: VoiceGenerator;
    private constructor() { super(); }
    static getInstance(): VoiceGenerator {
        if (!VoiceGenerator.instance) {
            VoiceGenerator.instance = new VoiceGenerator();
        }
        return VoiceGenerator.instance;
    }

    generate(): string {
        return `// Voice Generator - Speech-to-text, text-to-speech, commands
// Generated by Shadow AI

// Speech Recognition Hook
export function useSpeechRecognition() {
    const [transcript, setTranscript] = useState('');
    const [isListening, setIsListening] = useState(false);
    const recognitionRef = useRef<SpeechRecognition | null>(null);
    
    useEffect(() => {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (SpeechRecognition) {
            recognitionRef.current = new SpeechRecognition();
            recognitionRef.current.continuous = true;
            recognitionRef.current.interimResults = true;
            
            recognitionRef.current.onresult = (event) => {
                const current = event.resultIndex;
                const result = event.results[current];
                setTranscript(result[0].transcript);
            };
            
            recognitionRef.current.onend = () => setIsListening(false);
        }
    }, []);
    
    const startListening = () => {
        if (recognitionRef.current) {
            recognitionRef.current.start();
            setIsListening(true);
        }
    };
    
    const stopListening = () => {
        if (recognitionRef.current) {
            recognitionRef.current.stop();
            setIsListening(false);
        }
    };
    
    return { transcript, isListening, startListening, stopListening };
}

// Text-to-Speech
export function useTTS() {
    const speak = (text: string, options?: { rate?: number; pitch?: number; voice?: string }) => {
        const utterance = new SpeechSynthesisUtterance(text);
        
        if (options?.rate) utterance.rate = options.rate;
        if (options?.pitch) utterance.pitch = options.pitch;
        if (options?.voice) {
            const voices = speechSynthesis.getVoices();
            const voice = voices.find(v => v.name === options.voice);
            if (voice) utterance.voice = voice;
        }
        
        speechSynthesis.speak(utterance);
    };
    
    const stop = () => speechSynthesis.cancel();
    const pause = () => speechSynthesis.pause();
    const resume = () => speechSynthesis.resume();
    
    return { speak, stop, pause, resume };
}

// Voice Commands
class VoiceCommandHandler {
    private commands: Map<string, () => void> = new Map();
    
    register(phrase: string, handler: () => void) {
        this.commands.set(phrase.toLowerCase(), handler);
    }
    
    handle(transcript: string) {
        const normalized = transcript.toLowerCase().trim();
        
        for (const [phrase, handler] of this.commands) {
            if (normalized.includes(phrase)) {
                handler();
                return true;
            }
        }
        return false;
    }
}

// Whisper API Integration
class WhisperService {
    async transcribe(audioBlob: Blob): Promise<string> {
        const formData = new FormData();
        formData.append('file', audioBlob, 'audio.webm');
        formData.append('model', 'whisper-1');
        
        const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
            method: 'POST',
            headers: { 'Authorization': \`Bearer \${process.env.OPENAI_API_KEY}\` },
            body: formData
        });
        
        const data = await response.json();
        return data.text;
    }
}

export { VoiceCommandHandler, WhisperService };
`;
    }
}

export const voiceGenerator = VoiceGenerator.getInstance();
